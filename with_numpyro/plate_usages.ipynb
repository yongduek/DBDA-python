{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "choice-journal",
   "metadata": {},
   "source": [
    "http://pyro.ai/examples/svi_part_i.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-proportion",
   "metadata": {},
   "source": [
    "To turn this into a probabilistic model we encode heads and tails as 1s and 0s. We encode the fairness of the coin as a real number f, where f satisfies f∈[0.0,1.0] and f=0.50 corresponds to a perfectly fair coin. Our prior belief about f will be encoded by a beta distribution, specifically Beta(10,10), which is a symmetric probability distribution on the interval [0.0,1.0] that is peaked at f=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "verified-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-aurora",
   "metadata": {},
   "source": [
    "Here we have a single latent random variable ('latent_fairness'), which is distributed according to Beta(10,10). Conditioned on that random variable, we observe each of the datapoints using a bernoulli likelihood. Note that each observation is assigned a unique name in Pyro.\n",
    "\n",
    "Our next task is to define a corresponding guide, i.e. an appropriate variational distribution for the latent random variable f. The only real requirement here is that q(f) should be a probability distribution over the range [0.0,1.0], since f doesn’t make sense outside of that range. A simple choice is to use another beta distribution parameterized by two trainable parameters αq and βq. Actually, in this particular case this is the ‘right’ choice, since conjugacy of the bernoulli and beta distributions means that the exact posterior is a beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "funded-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro.\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0), constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0), constraint=constraints.positive)\n",
    "    \n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-warehouse",
   "metadata": {},
   "source": [
    "There are a few things to note here:\n",
    "\n",
    "- We’ve taken care that the names of the random variables line up exactly between the model and guide.\n",
    "\n",
    "- `model(data)` and `guide(data)` take the same arguments.\n",
    "\n",
    "- The variational parameters are `torch.tensors`. The requires_grad flag is automatically set to True by pyro.param.\n",
    "\n",
    "- We use `constraint=constraints.positive` to ensure that `alpha_q` and `beta_q` remain non-negative during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-panama",
   "metadata": {},
   "source": [
    "Now we can proceed to do stochastic variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "infinite-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import Adam\n",
    "\n",
    "# set up the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hawaiian-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_report(mymodel):\n",
    "    from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "    # setup the inference algorithm\n",
    "    svi = SVI(mymodel, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "    data = torch.tensor([1.]*10 + [0.]*5)\n",
    "    print(data)\n",
    "\n",
    "    n_steps = 5000\n",
    "    # do gradient steps\n",
    "    for step in range(n_steps):\n",
    "        svi.step(data)\n",
    "\n",
    "    # grab the learned variational parameters\n",
    "    alpha_q = pyro.param(\"alpha_q\").item()\n",
    "    beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "    print(f'From SVI: alpha_q = {alpha_q:.2f}  beta_q = {beta_q:.2f}')\n",
    "\n",
    "    # here we use some facts about the beta distribution\n",
    "    # compute the inferred mean of the coin's fairness\n",
    "    inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "    # compute inferred standard deviation\n",
    "    factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "    inferred_std = inferred_mean * np.sqrt(factor)\n",
    "\n",
    "    print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "          \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acquired-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
      "From SVI: alpha_q = 19.01  beta_q = 14.11\n",
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.574 +- 0.085\n"
     ]
    }
   ],
   "source": [
    "run_and_report(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-playback",
   "metadata": {},
   "source": [
    "---\n",
    "http://pyro.ai/examples/svi_part_ii.html\n",
    "\n",
    "Making Conditional Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "characteristic-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first model used above\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-cinema",
   "metadata": {},
   "source": [
    "**Sequential plate.**\n",
    "For this model the observations are conditionally independent given the latent random variable `latent_fairness`. To explicitly mark this in Pyro we basically just need to replace the Python builtin `range` with the Pyro construct `plate`:\n",
    "\n",
    "We see that `pyro.plate` is very similar to `range` with one main difference: each invocation of `plate` requires the user to provide a unique name. The second argument is an integer just like for `range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "varied-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    alpha0, beta0 = torch.tensor(10.0), torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data [WE ONLY CHANGE THE NEXT LINE]\n",
    "    for i in pyro.plate(\"data_loop\", len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "accurate-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
      "From SVI: alpha_q = 19.43  beta_q = 14.38\n",
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.575 +- 0.084\n"
     ]
    }
   ],
   "source": [
    "run_and_report(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-designation",
   "metadata": {},
   "source": [
    "**Vectorized plate**\n",
    "Conceptually vectorized plate is the same as sequential plate except that it is a vectorized operation (as torch.arange is to range). As such it potentially enables large speed-ups compared to the explicit for loop that appears with sequential plate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cheap-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    alpha0, beta0 = torch.tensor(10.0), torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data [WE ONLY CHANGE THE NEXT LINE]\n",
    "    with pyro.plate(\"data_loop\"):  # vectorized plate. No indexing is required.\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(f), obs=data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "perceived-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\n",
      "From SVI: alpha_q = 19.02  beta_q = 14.37\n",
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.570 +- 0.084\n"
     ]
    }
   ],
   "source": [
    "run_and_report(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-wholesale",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-newcastle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
